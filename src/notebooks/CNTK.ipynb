{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Tangent Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various papers have shown that, in the infinite width limit, any _Neural Network_ with _standard architecture_ is equivalent to a Gaussian process at initialization. Furthermore, Jacot and Al. have introduced the concept of _Neural Tangent Kernel_ and proved that for MLP, it stays constant during training.\n",
    "\n",
    "Although, for MLPs, regression based on NTK obtains results equivalent, or even better, than those of the original network, the same does not holds for CNNs, where experimental results show that the actual network greatly outperform the kernel methods.\n",
    "\n",
    "We investigate in this notebook the reason for that behaviour. In particular, we focus on what the network is actually learning, to confront it with what the NTK gives as a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The experiment\n",
    "\n",
    "We consider different network architectures and, for each, we study the learning of a Neural Network $NN$ randomly initialized.\n",
    "It is proved in [yang2019scaling] that, in the infinite width limit (IWL), each such Network is equivalent at initialization to a Gaussian Precess.\n",
    "Furthermore, [jacot2019neural] proved that in the MLP case during training, the gradient follow the Neural Tangent Kernel, which in the IWL is constant and converges.\n",
    "We conjecture that the same should hold for any architecture and study what the actual Net learns compared to the infinite width limit.\n",
    "\n",
    "In particular, we consider for each $NN$ at initialization, the actual Neural Tangent Kernel $$???$$ which we denote by $NTK^M_0$ and its theoretical-infinite-wide counterpart $NTK^\\infty$.\n",
    "\n",
    "We then train the Net to get $NN_\\infty$ and its corresponding $NTK^M_\\infty$.\n",
    "By [jacot2019neural], we know that $NTK^\\infty$ does not change during training, so we can study the kernel regression functions $f^M_\\infty$ and $f^\\infty$ given respectively by the finite-case Tangent Kernel and the infinte-width one.\n",
    "Note that the subscript $\\infty$ refers to the training, while the superscript one refers to the number of parameters.\n",
    "\n",
    "We expect the $f^M_\\infty$ to be very close to the actual Net output $NN_\\infty$ and we want to understand the difference between it and the infinite width kernel regression function.\n",
    "\n",
    "We use **Pythorch** as our deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "To start, we work with the **MNIST** dataset, to investigate what the CNN kernel learns. When working with transformers we will use a semantic dataset **TBD**\n",
    "\n",
    "We use 1000 samples for train and we study the Kernel on a test of 100 samples, representing each class (digit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple design, to warm up the computations. The derivation of the NTK is then based on [arora2019exact].\n",
    "\n",
    "Our "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[jacot2019neural]: http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks\n",
    "[yang2019scaling]: https://arxiv.org/abs/1902.04760\n",
    "[arora2019exact]: https://arxiv.org/abs/1904.11955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
